{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our previous post on aligning span annotations to Hugginface's tokenizer outputs discussed the the various tradeoffs one needs to consider, and concluded that a windowing strategy over the tokenized text and labels is optimal for our use cases. \n",
    "\n",
    "This post demonstrates an end to end implementation of token alignment and windowing. We'll start by implementing utility classes that make programming a little easier, then implement the alignment functionality which aligns offset annotations to the out of a tokenizer. Finnaly we'll implement a PyTorch Dataset that stores our aligned tokens and labels as windows, a Collator to implement batching and a simple DataLoader to be used in training. \n",
    "\n",
    "We'll show and end to end flow on the DDI Corpus, recognizing pharmacological entities with BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Classes For Convenient APIs\n",
    "We'll start by defining some types and utility classes that will make our work more convient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List,Any\n",
    "IntList = List[int] # A list of token_ids\n",
    "IntListList = List[IntList] # A List of List of token_ids, e.g. a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Alignment Algorithm\n",
    "\n",
    "### FastTokenizers Simplify Alignment\n",
    "Recent versions of Hugginface's tokenizers library include variants of Tokenizers that end with Fast and inherit from [PreTrainedTokenizerFast](https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizerFast)  such as [BertTokenizerFast](https://huggingface.co/transformers/model_doc/bert.html#berttokenizerfast) and [GPT2TokenizerFast](https://huggingface.co/transformers/model_doc/gpt2.html#gpt2tokenizerfast). \n",
    "\n",
    "Per the tokenizer's documentation\n",
    "> When the tokenizer is a “Fast” tokenizer (i.e., backed by HuggingFace tokenizers library), [the output] provides in addition several advanced alignment methods which can be used to map between the original string (character and words) and the token space (e.g., getting the index of the token comprising a given character or the span of characters corresponding to a given token).\n",
    "\n",
    "Notably, the output provides the methods [token_to_chars](https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.BatchEncoding.token_to_chars) and [char_to_token](https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.BatchEncoding.char_to_token) which do exactly what their name implies, provide mappings between tokens and charachter offsets in the original text. That's exactly what we need to align annotations in offset format with tokens.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A warmup implementation\n",
    "Our final implementation will use the BIOUL scheme we mentioned before. But before we do that, let's try a simple alignment to see what it feels like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tal Perry Person\n",
      "founder Title\n",
      "LightTag Org\n"
     ]
    }
   ],
   "source": [
    "text = \"I am Tal Perry, founder of LightTag\"\n",
    "annotations = [\n",
    "    dict(start=5,end=14,text=\"Tal Perry\",label=\"Person\"),\n",
    "    dict(start=16,end=23,text=\"founder\",label=\"Title\"),\n",
    "    dict(start=27,end=35,text=\"LightTag\",label=\"Org\"),\n",
    "    \n",
    "              ]\n",
    "for anno in annotations:\n",
    "    # Show our annotations\n",
    "    print (text[anno['start']:anno['end']],anno['label'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast,  BatchEncoding\n",
    "from tokenizers import Encoding\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased') # Load a pre-trained tokenizer\n",
    "tokenized_batch : BatchEncoding = tokenizer(text)\n",
    "tokenized_text :Encoding  =tokenized_batch[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] - O\n",
      "I - O\n",
      "am - O\n",
      "Ta - Person\n",
      "##l - Person\n",
      "Perry - Person\n",
      ", - O\n",
      "founder - Title\n",
      "of - O\n",
      "Light - Org\n",
      "##T - Org\n",
      "##ag - Org\n",
      "[SEP] - O\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenized_text.tokens\n",
    "aligned_labels = [\"O\"]*len(tokens) # Make a list to store our labels the same length as our tokens\n",
    "for anno in (annotations):\n",
    "    for char_ix in range(anno['start'],anno['end']):\n",
    "        token_ix = tokenized_text.char_to_token(char_ix)\n",
    "        if token_ix is not None: # White spaces have no token and will return None\n",
    "            aligned_labels[token_ix] = anno['label']\n",
    "for token,label in zip(tokens,aligned_labels):\n",
    "    print (token,\"-\",label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accounting For Multi Token Annotations\n",
    "In the above example, some of our annotations spanned multiple tokens. For instance \"Tal Perry\" spanned \"Ta\", \"##l\" and \"Perry\". Clearly by themeselves none of those tokens are a Person, and so our current alignment scheme isn't as useful as it could be. \n",
    "To overcome that, we'll use the previously mentioned BIOLU scheme, which will indicate if a token is the begining, inside, last token in an annotation or if it is not part of an annotation or if it is perfectly aligned with an annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] - O\n",
      "I - O\n",
      "am - O\n",
      "Ta - B-Person\n",
      "##l - I-Person\n",
      "Perry - L-Person\n",
      ", - O\n",
      "founder - U-Title\n",
      "of - O\n",
      "Light - B-Org\n",
      "##T - I-Org\n",
      "##ag - L-Org\n",
      "[SEP] - O\n"
     ]
    }
   ],
   "source": [
    "def align_tokens_and_annotations_bilou(tokenized: Encoding, annotations):\n",
    "    tokens = tokenized.tokens\n",
    "    aligned_labels = [\"O\"] * len(\n",
    "        tokens\n",
    "    )  # Make a list to store our labels the same length as our tokens\n",
    "    for anno in annotations:\n",
    "        annotation_token_ix_set = (\n",
    "            set()\n",
    "        )  # A set that stores the token indices of the annotation\n",
    "        for char_ix in range(anno[\"start\"], anno[\"end\"]):\n",
    "\n",
    "            token_ix = tokenized.char_to_token(char_ix)\n",
    "            if token_ix is not None:\n",
    "                annotation_token_ix_set.add(token_ix)\n",
    "        if len(annotation_token_ix_set) == 1:\n",
    "            # If there is only one token\n",
    "            token_ix = annotation_token_ix_set.pop()\n",
    "            prefix = (\n",
    "                \"U\"  # This annotation spans one token so is prefixed with U for unique\n",
    "            )\n",
    "            aligned_labels[token_ix] = f\"{prefix}-{anno['label']}\"\n",
    "\n",
    "        else:\n",
    "\n",
    "            last_token_in_anno_ix = len(annotation_token_ix_set) - 1\n",
    "            for num, token_ix in enumerate(sorted(annotation_token_ix_set)):\n",
    "                if num == 0:\n",
    "                    prefix = \"B\"\n",
    "                elif num == last_token_in_anno_ix:\n",
    "                    prefix = \"L\"  # Its the last token\n",
    "                else:\n",
    "                    prefix = \"I\"  # We're inside of a multi token annotation\n",
    "                aligned_labels[token_ix] = f\"{prefix}-{anno['label']}\"\n",
    "    return aligned_labels\n",
    "\n",
    "\n",
    "labels = align_tokens_and_annotations_bilou(tokenized_text, annotations)\n",
    "for token, label in zip(tokens, labels):\n",
    "    print(token, \"-\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how **founder** above has a **U** prefix and the other annotations now follow a BIL scheme.\n",
    "\n",
    "**Note** In production, you'll convert the labels to ids, using the LabelSet we defined above. I'm going to skip that for now for the sake of readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Labels To Ids\n",
    "It's great that we have our annotations aligned, but we need the labels as integer ids for training. During inference, we'll also need a way to map predicted ids back to labels.\n",
    "I'm going to make a custom class that handles that, called a LabelSet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] - 0\n",
      "I - 0\n",
      "am - 0\n",
      "Ta - 1\n",
      "##l - 2\n",
      "Perry - 3\n",
      ", - 0\n",
      "founder - 12\n",
      "of - 0\n",
      "Light - 5\n",
      "##T - 6\n",
      "##ag - 7\n",
      "[SEP] - 0\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "class LabelSet:\n",
    "    def __init__(self, labels: List[str]):\n",
    "        self.labels_to_id = {}\n",
    "        self.ids_to_label = {}\n",
    "        self.labels_to_id[\"O\"] = 0\n",
    "        self.ids_to_label[0] = \"O\"\n",
    "        num = 0  # in case there are no labels\n",
    "        # Writing BILU will give us incremntal ids for the labels\n",
    "        for _num, (label, s) in enumerate(itertools.product(labels, \"BILU\")):\n",
    "            num = _num + 1  # skip 0\n",
    "            l = f\"{s}-{label}\"\n",
    "            self.labels_to_id[l] = num\n",
    "            self.ids_to_label[num] = l\n",
    "        # Add the OUTSIDE label - no label for the token\n",
    "\n",
    "    def get_aligned_label_ids_from_annotations(self, tokenized_text, annotations):\n",
    "        raw_labels = align_tokens_and_annotations_bilou(tokenized_text, annotations)\n",
    "        return list(map(self.labels_to_id.get, raw_labels))\n",
    "\n",
    "\n",
    "example_label_set = LabelSet(labels=[\"Person\", \"Org\", \"Title\"])\n",
    "aligned_label_ids = example_label_set.get_aligned_label_ids_from_annotations(\n",
    "    tokenized_text, annotations\n",
    ")\n",
    "\n",
    "for token, label in zip(tokens, aligned_label_ids):\n",
    "    print(token, \"-\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching\n",
    "Now that we have alignment logic in place, we need to figure out how to load, batch and pad the data. We also need to habdle the case where our text is longer than we can feed our model. Below we show an implementation of a particular strategy, windowing over uniform length segments of the text. This isn't the only strategy, or even necasarily the best, but it fits our use case well. You can read more about why [we use windowing when training ner models with BERT here](https://www.lighttag.io/blog/sequence-labeling-with-transformers/). Below we'll just show how to do that.\n",
    "\n",
    "## The Raw Dataset\n",
    "We'll be using the [DDI Corpus](https://www.sciencedirect.com/science/article/pii/S1532046413001123). This notebook will pull the files locally but you can download them as [JSON here](https://github.com/LightTag/DDICorpus).\n",
    "Let's take a quick look at it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'annotations': [{'end': 58, 'label': 'drug', 'start': 47, 'tag': 'drug'},\n",
      "                 {'end': 75, 'label': 'drug', 'start': 62, 'tag': 'drug'},\n",
      "                 {'end': 135, 'label': 'drug', 'start': 124, 'tag': 'drug'},\n",
      "                 {'end': 164, 'label': 'drug', 'start': 152, 'tag': 'drug'}],\n",
      " 'content': 'Pharmacokinetic studies have demonstrated that omeprazole and '\n",
      "            'erythromycin significantly increased the systemic exposure of '\n",
      "            'cilostazol and/or its major metabolites.',\n",
      " 'metadata': {'original_id': 'DrugDDI.d452.s1'}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "raw = json.load(open(\"./ddi_train.json\"))\n",
    "for example in raw:\n",
    "    # our simple implementation expects the label to be called label, so we adjust the original data\n",
    "    for anno in example[\"annotations\"]:\n",
    "        anno[\"label\"] = anno[\"tag\"]\n",
    "pprint(raw[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at that tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] - O\n",
      "Ph - O\n",
      "##arma - O\n",
      "##co - O\n",
      "##kin - O\n",
      "##etic - O\n",
      "studies - O\n",
      "have - O\n",
      "demonstrated - O\n",
      "that - O\n",
      "o - B-drug\n",
      "##me - I-drug\n",
      "##pra - I-drug\n",
      "##zo - I-drug\n",
      "##le - L-drug\n",
      "and - O\n",
      "er - B-drug\n",
      "##yt - I-drug\n",
      "##hr - I-drug\n",
      "##omy - I-drug\n",
      "##cin - L-drug\n",
      "significantly - O\n",
      "increased - O\n",
      "the - O\n",
      "systemic - O\n",
      "exposure - O\n",
      "of - O\n",
      "c - B-drug\n",
      "##ilo - I-drug\n",
      "##sta - I-drug\n",
      "##zo - I-drug\n",
      "##l - L-drug\n",
      "and - O\n",
      "/ - O\n",
      "or - O\n",
      "its - O\n",
      "major - O\n",
      "meta - B-drug\n",
      "##bol - I-drug\n",
      "##ites - I-drug\n",
      ". - L-drug\n",
      "[SEP] - O\n"
     ]
    }
   ],
   "source": [
    "example = raw[2]\n",
    "tokenized_batch = tokenizer(example[\"content\"])\n",
    "tokenized_text = tokenized_batch[0]\n",
    "labels = align_tokens_and_annotations_bilou(tokenized_text, example[\"annotations\"])\n",
    "for token, label in zip(tokenized_text.tokens, labels):\n",
    "    print(token, \"-\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Padding and Windowing in a Dataset\n",
    "Our dataset is conveniently split into sentences. We still need to batch it and pad the examples. More commonly, data is not split into sentences, and so we will window over fixed sized parts of it. The windowing, padding and alignment logic will be done in a pytorch Dataset and we'll get to batching in a moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingExample:\n",
    "    input_ids: IntList\n",
    "    attention_masks: IntList\n",
    "    labels: IntList\n",
    "\n",
    "\n",
    "class TraingDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: Any,\n",
    "        label_set: LabelSet,\n",
    "        tokenizer: PreTrainedTokenizerFast,\n",
    "        tokens_per_batch=32,\n",
    "        window_stride=None,\n",
    "    ):\n",
    "        self.label_set = label_set\n",
    "        if window_stride is None:\n",
    "            self.window_stride = tokens_per_batch\n",
    "        self.tokenizer = tokenizer\n",
    "        for example in data:\n",
    "            # changes tag key to label\n",
    "            for a in example[\"annotations\"]:\n",
    "                a[\"label\"] = a[\"tag\"]\n",
    "        self.texts = []\n",
    "        self.annotations = []\n",
    "\n",
    "        for example in data:\n",
    "            self.texts.append(example[\"content\"])\n",
    "            self.annotations.append(example[\"annotations\"])\n",
    "        ###TOKENIZE All THE DATA\n",
    "        tokenized_batch = self.tokenizer(self.texts, add_special_tokens=False)\n",
    "        ###ALIGN LABELS ONE EXAMPLE AT A TIME\n",
    "        aligned_labels = []\n",
    "        for ix in range(len(tokenized_batch.encodings)):\n",
    "            encoding = tokenized_batch.encodings[ix]\n",
    "            raw_annotations = self.annotations[ix]\n",
    "            aligned = label_set.get_aligned_label_ids_from_annotations(\n",
    "                encoding, raw_annotations\n",
    "            )\n",
    "            aligned_labels.append(aligned)\n",
    "        ###END OF LABEL ALIGNMENT\n",
    "\n",
    "        ###MAKE A LIST OF TRAINING EXAMPLES. (This is where we add padding)\n",
    "        self.training_examples: List[TrainingExample] = []\n",
    "        empty_label_id = \"O\"\n",
    "        for encoding, label in zip(tokenized_batch.encodings, aligned_labels):\n",
    "            length = len(label)  # How long is this sequence\n",
    "            for start in range(0, length, self.window_stride):\n",
    "\n",
    "                end = min(start + tokens_per_batch, length)\n",
    "\n",
    "                # How much padding do we need ?\n",
    "                padding_to_add = max(0, tokens_per_batch - end + start)\n",
    "                self.training_examples.append(\n",
    "                    TrainingExample(\n",
    "                        # Record the tokens\n",
    "                        input_ids=encoding.ids[start:end]  # The ids of the tokens\n",
    "                        + [self.tokenizer.pad_token_id]\n",
    "                        * padding_to_add,  # padding if needed\n",
    "                        labels=(\n",
    "                            label[start:end]\n",
    "                            + [-100] * padding_to_add  # padding if needed\n",
    "                        ),  # -100 is a special token for padding of labels,\n",
    "                        attention_masks=(\n",
    "                            encoding.attention_mask[start:end]\n",
    "                            + [0]\n",
    "                            * padding_to_add  # 0'd attenetion masks where we added padding\n",
    "                        ),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.training_examples)\n",
    "\n",
    "    def __getitem__(self, idx) -> TrainingExample:\n",
    "\n",
    "        return self.training_examples[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's See what comes out\n",
    "Below we'll create a dataset instance.\n",
    "We first create a label_set, in this case there is only one label, **drug**. \n",
    "We then instantiate our Dataset by passing the raw data, the tokenizer and the label_set.\n",
    "We get back **TrainingExample** instances with the windowed and padded  input_ids and label_ids as well as attention_masks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingExample(input_ids=[1233, 1621, 4420, 18061, 5165, 1114, 4267, 6066, 1465, 3171, 1306, 117, 1126, 27558, 1104, 140], attention_masks=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], labels=[3, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 3, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "label_set = LabelSet(labels=[\"drug\"])\n",
    "ds = TraingDataset(\n",
    "    data=raw, tokenizer=tokenizer, label_set=label_set, tokens_per_batch=16\n",
    ")\n",
    "ex = ds[10]\n",
    "pprint(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching\n",
    "We still need a way batch these examples. We can't feed a list of TraingExamples to a model, we need to make tensors out of the input_ids and labels. This is easily achieved with a collating function. A collating function gets a list of items from our dataset (in our case a list of TraingExamples) and returns a batched tensors. \n",
    "\n",
    "We'll simplify things, by making a **TraingBatch** class whose constructor is the collating function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class TraingingBatch:\n",
    "    def __getitem__(self, item):\n",
    "        return getattr(self, item)\n",
    "\n",
    "    def __init__(self, examples: List[TrainingExample]):\n",
    "        self.input_ids: torch.Tensor\n",
    "        self.attention_masks: torch.Tensor\n",
    "        self.labels: torch.Tensor\n",
    "        input_ids: IntListList = []\n",
    "        masks: IntListList = []\n",
    "        labels: IntListList = []\n",
    "        for ex in examples:\n",
    "            input_ids.append(ex.input_ids)\n",
    "            masks.append(ex.attention_masks)\n",
    "            labels.append(ex.labels)\n",
    "        self.input_ids = torch.LongTensor(input_ids)\n",
    "        self.attention_masks = torch.LongTensor(masks)\n",
    "        self.labels = torch.LongTensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traing Our Model\n",
    "With our batching ready, let's use a pre trained model and show how to fine tune it on our new dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6987, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6388, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6135, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4385, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5159, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4509, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3011, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2812, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1388, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4184, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3591, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2249, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9483, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2650, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1502, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5125, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9448, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2908, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8918, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0335, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2265, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5571, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-cased\", num_labels=len(ds.label_set.ids_to_label.values())\n",
    ")\n",
    "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    ds,\n",
    "    collate_fn=TraingingBatch,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    ")\n",
    "for num, batch in enumerate(dataloader):\n",
    "    loss, logits = model(\n",
    "        input_ids=batch.input_ids,\n",
    "        attention_mask=batch.attention_masks,\n",
    "        labels=batch.labels,\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)\n",
    "    if num > 20:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
